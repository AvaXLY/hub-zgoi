1. 正则表达式 (Regex-Rule) 
优点
1. 绝对可控： 规则完全由开发者定义，输出结果高度确定、可预测，没有“黑盒”问题。
2. 速度极快： 匹配速度是纳秒/微秒级别，计算开销极小，对系统资源占用几乎可忽略。
3. 无需训练数据： 不需要任何标注数据，直接依赖领域知识和专家经验编写规则。
4. 可解释性极强： 任何匹配结果都可以追溯到具体的规则条⽂，非常容易调试和验证。


缺点
1. 维护成本高： 规则会随着需求变化而变得复杂且难以维护，容易形成“打补丁”式的规则库。
2. 泛化能力差： 只能匹配预设的模式，无法处理未见过的表达、歧义或细微的变形。
3. 难以覆盖所有情况： 自然语言灵活多变，穷举所有规则模式几乎是不可能的任务。
4. 依赖专家： 规则的质量高度依赖于编写者的领域知识和对Regex的掌握程度。

适用场景
格式固定的文本提取（如电话号码、身份证号）、简单的关键字或模式匹配、日志文件分析、数据清洗预处理。


2. TF-IDF	
优点
1. 简单高效： 概念简单，实现容易，计算速度很快，适合作为基线模型。
2. 无监督/轻量化： 不需要标注数据即可提取文本关键词，模型非常轻量。
3. 可解释性较好： 可以清楚地看到是哪些高权重的关键词对结果贡献最大。
4. 理论基础扎实： 有直观的统计学意义（词频越高，文档内重要性越高；文档频率越高，词语区分度越低）。

缺点
1. 无法捕捉语义： 纯粹基于词频统计，无法理解词语的上下文语境和语义信息（“苹果”手机 vs “苹果”水果）。
2. 稀疏性高： 容易产生高维稀疏向量，导致计算和存储效率降低。
3. 忽略词序： 基于词袋模型，完全忽略了词语在句子中的顺序信息。
4. 精度有限： 在复杂的NLP任务（如情感分析、机器翻译）上性能远不如现代深度模型。

适用场景
搜索引擎关键词加权、简单文档相似度计算、关键词提取、作为机器学习模型（如SVM）的文本特征，资源受限,并发高，快速响应的场景。


3. BERT	
优点
1. 强大的语义理解： 基于Transformer架构，能深度理解上下文语境，捕捉一词多义，表征能力极强。
2. 泛化性能好： 在海量数据上预训练后，通过微调即可在众多下游任务上取得state-of-the-art（最先进）的效果。
3. 特征丰富： 生成的词向量是深度上下文相关的，包含丰富的语义和语法信息。
4. 减少特征工程： 只需简单的输出层调整，即可应用于分类、QA、标注等多种任务。

缺点
1. 计算资源消耗大： 模型参数量巨大（数亿甚至数十亿），训练和推理需要大量的计算资源（GPU）和时间。
2. 可解释性差： 是典型的“黑盒”模型，难以解释其内部决策过程和具体依据。
3. 需要微调： 虽然预训练模型通用，但要用于特定任务，通常仍需大量的任务特定数据进行微调。
4. 部署成本高： 大规模模型对线上服务的部署和响应延迟提出了挑战。

适用场景
文本分类、情感分析、智能问答、语义相似度计算、命名实体识别、机器阅读理解等几乎所有NLP任务。

4. Prompt（提示词工程）	
优点
1. 高效利用预训练模型： 通过设计合适的提示，能更好地激发大模型的已有知识，避免全量微调。
2. 少样本/零样本学习： 在标注数据极少甚至没有的情况下，也能通过提示让模型完成新任务，泛化能力极强。
3. 更接近人类交互： 以自然语言的形式进行任务描述，降低了使用AI模型的门槛。
4. 统一多种任务： 可以将不同任务（如分类、生成）都转化为文本生成任务，范式统一。

缺点
1. 提示设计敏感： 模型性能高度依赖于提示词的设计，需要大量的试探和验证（Prompt Engineering）。
2. 结果不稳定： 提示词的微小改动可能导致输出结果的巨大差异，稳定性较差。
3. 依赖超大规模模型： 这种范式的有效性严重依赖于千亿级别参数的超大语言模型（如GPT-3/4）。
4. 可控性风险： 模型可能会生成不符合预期或带有偏见的内容，可控性不如传统方法。

适用场景
与大语言模型交互（如ChatGPT）、开放式文本生成、零样本/少样本学习任务、代码生成、创意写作。
